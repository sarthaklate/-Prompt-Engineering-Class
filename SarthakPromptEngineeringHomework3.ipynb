{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create 5 examples of word arithmetic similar to the \"king - man + woman ≈ queen\" analogy. Use words that have relevant semantic relationships. Steps\n",
        "\n",
        "Load the BERT model and tokenizer\n",
        "Implement functions to get word embeddings and perform word arithmetic.\n",
        "Write word_arithmetic and find_most_similar functions to create your examples\n",
        "The word arithmetic function will be able to take two list of words: ○ The first list is parameters to the word_arithmatic as example, (paris, france, italy), run the arithmetic and collect the return value (e.g., paris - france + italy = ?). ○ Using the find_most_similar function with return value of word_arithmetic as input, along with the second list of words like (rome, romaine, ramania, ronnie, random) to find the most similar word to the answer. ○ Show this for of 5 potential pairs of such words ○ Print answer for each of the 5 test cases."
      ],
      "metadata": {
        "id": "VFt9Gwi_Snbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 2. Get word embeddings\n",
        "def get_word_embedding(word):\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    # Take the embeddings of the [CLS] token\n",
        "    return outputs.last_hidden_state[0][0].detach().numpy()\n",
        "\n",
        "# 3. Word arithmetic function\n",
        "def word_arithmetic(word1, word2, word3):\n",
        "    embedding1 = get_word_embedding(word1)\n",
        "    embedding2 = get_word_embedding(word2)\n",
        "    embedding3 = get_word_embedding(word3)\n",
        "\n",
        "    # Word arithmetic: word1 - word2 + word3\n",
        "    result_vector = embedding1 - embedding2 + embedding3\n",
        "    return result_vector\n",
        "\n",
        "# 4. Find the most similar word from a list of candidates\n",
        "def find_most_similar(vector, candidates):\n",
        "    similarities = []\n",
        "    for word in candidates:\n",
        "        candidate_embedding = get_word_embedding(word)\n",
        "        similarity = cosine_similarity([vector], [candidate_embedding])[0][0]\n",
        "        similarities.append((word, similarity))\n",
        "    # Sort by highest similarity\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "# 5. Test with example pairs\n",
        "def test_word_arithmetic():\n",
        "    test_cases = [\n",
        "        (['teacher', 'school', 'classroom'], ['student', 'principal', 'playground', 'library']),\n",
        "        (['doctor', 'hospital', 'clinic'], ['patient', 'nurse', 'treatment', 'lady']),\n",
        "        (['author', 'publisher', 'bookstore'], ['nurse', 'book', 'reader', 'prince']),\n",
        "        (['chief', 'restaurant', 'kitchen'], ['food', 'recipe', 'salt', 'pendrive']),\n",
        "        (['artist', 'gallery', 'museum '], ['paint', 'exhibit', 'sun', 'twilight'])\n",
        "    ]\n",
        "\n",
        "    for test_input, candidates in test_cases:\n",
        "        result_vector = word_arithmetic(*test_input)\n",
        "        most_similar_word = find_most_similar(result_vector, candidates)\n",
        "        print(f\"Word Arithmetic: {test_input[0]} - {test_input[1]} + {test_input[2]} ≈ {most_similar_word}\")\n",
        "\n",
        "# Run the test cases\n",
        "test_word_arithmetic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erEv4MFCTIdw",
        "outputId": "8780c79f-3510-4fc1-b2cc-336c03ab22a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Arithmetic: teacher - school + classroom ≈ principal\n",
            "Word Arithmetic: doctor - hospital + clinic ≈ nurse\n",
            "Word Arithmetic: author - publisher + bookstore ≈ book\n",
            "Word Arithmetic: chief - restaurant + kitchen ≈ food\n",
            "Word Arithmetic: artist - gallery + museum  ≈ exhibit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a simple RAG system using LangChain, process an article of your choice, and run 5 different queries on its content. Steps\n",
        "\n",
        "1.Choose at least 5 diverse articles on a different topic of your interest from wikipedia dump on HuggingFace (e.g., Artificial Intelligence, Machine Learning, etc.).\n",
        "\n",
        "\n",
        "2.Use the provided code from the class to load and process each article, create embeddings, store embeddings for each article in the single VectorDB and set up the RAG system.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3.Formulate 10 diverse queries that explore various aspects of your article's content.\n",
        "Run each query using the run_query function and record the results."
      ],
      "metadata": {
        "id": "T3tEwewBev92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install langchain faiss-cpu transformers\n",
        "!pip install langchain\n",
        "import langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7cq919RfHe1",
        "outputId": "c5c5d9ad-1b64-410a-9a98-dd561cba9af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.121)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.121)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import LangChain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFaceLLM\n",
        "\n",
        "# Initialize HuggingFace Embeddings and LLM\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "llm_model_name = \"distilgpt2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "llm = HuggingFaceLLM(model_name=llm_model_name)\n",
        "\n",
        "# Create a vector store\n",
        "vector_store = FAISS(embedding_dim=embeddings.get_embedding_dim())\n",
        "\n",
        "# Load articles (assuming they are text files in the directory \"articles\")\n",
        "def load_articles(directory):\n",
        "    articles = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                articles.append(file.read())\n",
        "    return articles\n",
        "\n",
        "articles = load_articles(\"articles\")\n",
        "\n",
        "# Process articles and add them to the vector store\n",
        "for article in articles:\n",
        "    embedding = embeddings.embed(article)\n",
        "    vector_store.add_item(embedding, article)\n",
        "\n",
        "# Define the RAG system\n",
        "rag = LangChain(vector_store=vector_store, llm=llm)\n",
        "\n",
        "# Formulate queries\n",
        "queries = [\n",
        "    \"What is Machine Learning and how does it work?\",\n",
        "    \"What are the different types of Machine Learning algorithms?\",\n",
        "    \"How does supervised learning differ from unsupervised learning?\",\n",
        "    \"What is a neural network and how is it used in Machine Learning?\",\n",
        "    \"What are some common applications of Machine Learning in healthcare?\",\n",
        "    \"What challenges do practitioners face when deploying Machine Learning models in production?\",\n",
        "    \"How do hyperparameters affect the performance of Machine Learning models?\",\n",
        "    \"What are some recent advancements in Machine Learning research?,\n",
        "    \"How does Machine Learning contribute to autonomous vehicles?\"\",\n",
        "    \"What ethical considerations are associated with the use of Machine Learning?\"\n",
        "]\n",
        "\n",
        "# Run queries and record results\n",
        "def run_queries(rag, queries):\n",
        "    results = {}\n",
        "    for query in queries:\n",
        "        result = rag.run_query(query)\n",
        "        results[query] = result\n",
        "    return results\n",
        "\n",
        "results = run_queries(rag, queries)\n",
        "\n",
        "# Print results\n",
        "for query, result in results.items():\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "ozy11dmbkgnX",
        "outputId": "0913a1aa-5beb-4f29-cc43-e1cd0a3c8c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'LangChain' from 'langchain' (/usr/local/lib/python3.10/dist-packages/langchain/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-eabb22070819>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLangChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LangChain' from 'langchain' (/usr/local/lib/python3.10/dist-packages/langchain/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries1 = [\n",
        "    \"What is deep learning and how does it differ from traditional Machine Learning?\"\n",
        "    \"How do convolutional neural networks (CNNs) work and what are they used for?\"\n",
        "    \"What are recurrent neural networks (RNNs) and how are they applied in sequence data?\"\n",
        "    \"What is transfer learning and how can it be applied to improve model performance?\"\n",
        "    \"How do generative adversarial networks (GANs) work and what are their applications?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Run queries and record results\n",
        "def run_queries(rag, queries1):\n",
        "    results = {}\n",
        "    for query in queries1:\n",
        "        result = rag.run_query(query)\n",
        "        results[query] = result\n",
        "    return results\n",
        "\n",
        "results = run_queries(rag, queries)\n",
        "\n",
        "# Print results\n",
        "for query, result in results.items():\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result}\\n\")\n"
      ],
      "metadata": {
        "id": "h507AwohoQVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries2 = [\n",
        "    \"How is Machine Learning used in medical diagnosis and prognosis?\"\n",
        "    \"What are the benefits and challenges of using Machine Learning for personalized medicine?\"\n",
        "    \"How do Machine Learning models contribute to drug discovery and development?\"\n",
        "    \"What are the ethical considerations of using Machine Learning in healthcare?\"\n",
        "    \"How does Machine Learning enhance medical imaging and analysis?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Run queries and record results\n",
        "def run_queries(rag, queries2):\n",
        "    results = {}\n",
        "    for query in queries2:\n",
        "        result = rag.run_query(query)\n",
        "        results[query] = result\n",
        "    return results\n",
        "\n",
        "results = run_queries(rag, queries2)\n",
        "\n",
        "# Print results\n",
        "for query, result in results.items():\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result}\\n\")"
      ],
      "metadata": {
        "id": "b8X53zL3pJks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries3 = [\n",
        "    \"What are the key ethical concerns in Machine Learning and artificial intelligence?\"\n",
        "    \"How can bias in Machine Learning models be detected and mitigated?\"\n",
        "    \"What is explainable AI (XAI) and why is it important?\"\n",
        "    \"How does privacy impact Machine Learning practices and what are some solutions?\"\n",
        "    \"What are the implications of AI decision-making on societal fairness?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Run queries and record results\n",
        "def run_queries(rag, queries3):\n",
        "    results = {}\n",
        "    for query in queries3:\n",
        "        result = rag.run_query(query)\n",
        "        results[query] = result\n",
        "    return results\n",
        "\n",
        "results = run_queries(rag, queries3)\n",
        "\n",
        "# Print results\n",
        "for query, result in results.items():\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "0YyT1nG6pgZV",
        "outputId": "79beb638-e5d9-42b0-a7ec-e751e8811d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4d69de06df6e>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rag' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries4 = [\n",
        "    \"How is AI transforming job roles in different industries?\"\n",
        "    \"What are the potential benefits of AI for enhancing employee productivity?\"\n",
        "    \"How might AI-driven automation affect job displacement and creation?\"\n",
        "    \"What skills are increasingly important for workers in an AI-driven job market?\n",
        "    \"How can businesses effectively integrate AI technologies while managing workforce transitions?\"\n",
        "    \"What ethical considerations arise from using AI in hiring and recruitment processes?\"\n",
        "    \"How can employees upskill and reskill to remain competitive in an AI-dominated job market?\"\n",
        "    \"What role can AI play in improving workplace safety and reducing occupational hazards?\"\n",
        "    \"How is AI being used to personalize employee training and development program?\"\n",
        "    \"What are the potential long-term impacts of AI on job satisfaction and work-life balance?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Run queries and record results\n",
        "def run_queries(rag, queries3):\n",
        "    results = {}\n",
        "    for query in queries3:\n",
        "        result = rag.run_query(query)\n",
        "        results[query] = result\n",
        "    return results\n",
        "\n",
        "results = run_queries(rag, queries3)\n",
        "\n",
        "# Print results\n",
        "for query, result in results.items():\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result}\\n\")"
      ],
      "metadata": {
        "id": "Ee_bJdHHqBSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}